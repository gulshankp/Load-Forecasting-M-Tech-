{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3055b128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>nat_demand</th>\n",
       "      <th>T2M_toc</th>\n",
       "      <th>QV2M_toc</th>\n",
       "      <th>TQL_toc</th>\n",
       "      <th>W2M_toc</th>\n",
       "      <th>T2M_san</th>\n",
       "      <th>QV2M_san</th>\n",
       "      <th>TQL_san</th>\n",
       "      <th>W2M_san</th>\n",
       "      <th>T2M_dav</th>\n",
       "      <th>QV2M_dav</th>\n",
       "      <th>TQL_dav</th>\n",
       "      <th>W2M_dav</th>\n",
       "      <th>Holiday_ID</th>\n",
       "      <th>holiday</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03-01-2015 01:00</td>\n",
       "      <td>970.3450</td>\n",
       "      <td>25.865259</td>\n",
       "      <td>0.018576</td>\n",
       "      <td>0.016174</td>\n",
       "      <td>21.850546</td>\n",
       "      <td>23.482446</td>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>10.328949</td>\n",
       "      <td>22.662134</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>5.364148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03-01-2015 02:00</td>\n",
       "      <td>912.1755</td>\n",
       "      <td>25.899255</td>\n",
       "      <td>0.018653</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>22.166944</td>\n",
       "      <td>23.399255</td>\n",
       "      <td>0.017265</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>10.681517</td>\n",
       "      <td>22.578943</td>\n",
       "      <td>0.016509</td>\n",
       "      <td>0.087646</td>\n",
       "      <td>5.572471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-01-2015 03:00</td>\n",
       "      <td>900.2688</td>\n",
       "      <td>25.937280</td>\n",
       "      <td>0.018768</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>22.454911</td>\n",
       "      <td>23.343530</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>10.874924</td>\n",
       "      <td>22.531030</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.078735</td>\n",
       "      <td>5.871184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03-01-2015 04:00</td>\n",
       "      <td>889.9538</td>\n",
       "      <td>25.957544</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.016273</td>\n",
       "      <td>22.110481</td>\n",
       "      <td>23.238794</td>\n",
       "      <td>0.017128</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>10.518620</td>\n",
       "      <td>22.512231</td>\n",
       "      <td>0.016487</td>\n",
       "      <td>0.068390</td>\n",
       "      <td>5.883621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03-01-2015 05:00</td>\n",
       "      <td>893.6865</td>\n",
       "      <td>25.973840</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>0.017281</td>\n",
       "      <td>21.186089</td>\n",
       "      <td>23.075403</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>9.733589</td>\n",
       "      <td>22.481653</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>5.611724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48043</th>\n",
       "      <td>26-06-2020 20:00</td>\n",
       "      <td>1128.5592</td>\n",
       "      <td>27.246545</td>\n",
       "      <td>0.020303</td>\n",
       "      <td>0.055511</td>\n",
       "      <td>9.289304</td>\n",
       "      <td>25.715295</td>\n",
       "      <td>0.019746</td>\n",
       "      <td>0.121552</td>\n",
       "      <td>1.990773</td>\n",
       "      <td>23.746545</td>\n",
       "      <td>0.018381</td>\n",
       "      <td>0.150879</td>\n",
       "      <td>2.444658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48044</th>\n",
       "      <td>26-06-2020 21:00</td>\n",
       "      <td>1112.7488</td>\n",
       "      <td>27.099573</td>\n",
       "      <td>0.020395</td>\n",
       "      <td>0.053848</td>\n",
       "      <td>9.837504</td>\n",
       "      <td>25.552698</td>\n",
       "      <td>0.019632</td>\n",
       "      <td>0.153870</td>\n",
       "      <td>2.094459</td>\n",
       "      <td>23.693323</td>\n",
       "      <td>0.018320</td>\n",
       "      <td>0.156311</td>\n",
       "      <td>2.515814</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48045</th>\n",
       "      <td>26-06-2020 22:00</td>\n",
       "      <td>1081.5680</td>\n",
       "      <td>26.971155</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.057251</td>\n",
       "      <td>10.262464</td>\n",
       "      <td>25.393030</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>2.396369</td>\n",
       "      <td>23.658655</td>\n",
       "      <td>0.018327</td>\n",
       "      <td>0.153259</td>\n",
       "      <td>2.800717</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48046</th>\n",
       "      <td>26-06-2020 23:00</td>\n",
       "      <td>1041.6240</td>\n",
       "      <td>26.867487</td>\n",
       "      <td>0.020464</td>\n",
       "      <td>0.064178</td>\n",
       "      <td>10.326567</td>\n",
       "      <td>25.258112</td>\n",
       "      <td>0.019403</td>\n",
       "      <td>0.108063</td>\n",
       "      <td>2.720871</td>\n",
       "      <td>23.601862</td>\n",
       "      <td>0.018358</td>\n",
       "      <td>0.152771</td>\n",
       "      <td>3.138132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48047</th>\n",
       "      <td>27-06-2020 00:00</td>\n",
       "      <td>1013.5683</td>\n",
       "      <td>26.750330</td>\n",
       "      <td>0.020441</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>10.198346</td>\n",
       "      <td>25.125330</td>\n",
       "      <td>0.019281</td>\n",
       "      <td>0.100189</td>\n",
       "      <td>2.785751</td>\n",
       "      <td>23.562830</td>\n",
       "      <td>0.018404</td>\n",
       "      <td>0.162598</td>\n",
       "      <td>3.286878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48048 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               datetime  nat_demand    T2M_toc  QV2M_toc   TQL_toc    W2M_toc  \\\n",
       "0      03-01-2015 01:00    970.3450  25.865259  0.018576  0.016174  21.850546   \n",
       "1      03-01-2015 02:00    912.1755  25.899255  0.018653  0.016418  22.166944   \n",
       "2      03-01-2015 03:00    900.2688  25.937280  0.018768  0.015480  22.454911   \n",
       "3      03-01-2015 04:00    889.9538  25.957544  0.018890  0.016273  22.110481   \n",
       "4      03-01-2015 05:00    893.6865  25.973840  0.018981  0.017281  21.186089   \n",
       "...                 ...         ...        ...       ...       ...        ...   \n",
       "48043  26-06-2020 20:00   1128.5592  27.246545  0.020303  0.055511   9.289304   \n",
       "48044  26-06-2020 21:00   1112.7488  27.099573  0.020395  0.053848   9.837504   \n",
       "48045  26-06-2020 22:00   1081.5680  26.971155  0.020448  0.057251  10.262464   \n",
       "48046  26-06-2020 23:00   1041.6240  26.867487  0.020464  0.064178  10.326567   \n",
       "48047  27-06-2020 00:00   1013.5683  26.750330  0.020441  0.063965  10.198346   \n",
       "\n",
       "         T2M_san  QV2M_san   TQL_san    W2M_san    T2M_dav  QV2M_dav  \\\n",
       "0      23.482446  0.017272  0.001855  10.328949  22.662134  0.016562   \n",
       "1      23.399255  0.017265  0.001327  10.681517  22.578943  0.016509   \n",
       "2      23.343530  0.017211  0.001428  10.874924  22.531030  0.016479   \n",
       "3      23.238794  0.017128  0.002599  10.518620  22.512231  0.016487   \n",
       "4      23.075403  0.017059  0.001729   9.733589  22.481653  0.016456   \n",
       "...          ...       ...       ...        ...        ...       ...   \n",
       "48043  25.715295  0.019746  0.121552   1.990773  23.746545  0.018381   \n",
       "48044  25.552698  0.019632  0.153870   2.094459  23.693323  0.018320   \n",
       "48045  25.393030  0.019518  0.144531   2.396369  23.658655  0.018327   \n",
       "48046  25.258112  0.019403  0.108063   2.720871  23.601862  0.018358   \n",
       "48047  25.125330  0.019281  0.100189   2.785751  23.562830  0.018404   \n",
       "\n",
       "        TQL_dav   W2M_dav  Holiday_ID  holiday  school  \n",
       "0      0.096100  5.364148           0        0       0  \n",
       "1      0.087646  5.572471           0        0       0  \n",
       "2      0.078735  5.871184           0        0       0  \n",
       "3      0.068390  5.883621           0        0       0  \n",
       "4      0.064362  5.611724           0        0       0  \n",
       "...         ...       ...         ...      ...     ...  \n",
       "48043  0.150879  2.444658           0        0       1  \n",
       "48044  0.156311  2.515814           0        0       1  \n",
       "48045  0.153259  2.800717           0        0       1  \n",
       "48046  0.152771  3.138132           0        0       1  \n",
       "48047  0.162598  3.286878           0        0       1  \n",
       "\n",
       "[48048 rows x 17 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Data Preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Load the data\n",
    "data = pd.read_csv('continuous dataset.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474fa48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc95968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime format\n",
    "data['datetime'] = data['datetime'][:43776].apply(lambda x: datetime.strptime(x, '%d-%m-%Y %H:%M'))\n",
    "\n",
    "# Set the datetime as index\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "# Resample the data to hourly frequency\n",
    "data = data.resample('H').mean()\n",
    "\n",
    "# Create lag features\n",
    "for i in range(1, 25):\n",
    "    data['lag_{}'.format(i)] = data['nat_demand'].shift(i)\n",
    "\n",
    "# Create rolling mean and standard deviation features\n",
    "data['rolling_mean'] = data['nat_demand'].rolling(window=24).mean()\n",
    "data['rolling_std'] = data['nat_demand'].rolling(window=24).std()\n",
    "\n",
    "# Create weekday and hour features\n",
    "data['weekday'] = data.index.weekday\n",
    "data['hour'] = data.index.hour\n",
    "\n",
    "# Remove missing values\n",
    "data.dropna(inplace=True)\n",
    "train_size = int(len(data) * 0.8)\n",
    "train_data = data[:train_size]/1000\n",
    "test_data = data[train_size:]/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef9f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gulsh\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\gulsh\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\gulsh\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.22788039756857034\n",
      "MSE: 0.0813881454123888\n",
      "RMSE: 0.2852860764432586\n",
      "MAE = 0.23 (%)\n",
      "MAPE = 16.97 (%)\n",
      "RSME = 0.29 (%)\n"
     ]
    }
   ],
   "source": [
    "#ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit the ARIMA model\n",
    "model = ARIMA(train_data['nat_demand'], order=(5, 1, 0))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model_fit.predict(start=len(train_data), end=len(data)-1, typ='levels')\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(test_data['nat_demand'], predictions)\n",
    "mse = mean_squared_error(test_data['nat_demand'], predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('MAE:', mae)\n",
    "print('MSE:', mse)\n",
    "print('RMSE:', rmse)\n",
    "ypred=predictions\n",
    "y_test=test_data['nat_demand']\n",
    "y_arima=\n",
    "#For MAE\n",
    "mae=np.mean(np.abs(ypred-y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "def mean_absolute_percentage_error(y_test, ypred): \n",
    "    return np.mean(np.abs((y_test - ypred)/y_test))*100.\n",
    "mape = mean_absolute_percentage_error(y_test, ypred)\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da1dbf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gulsh\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\gulsh\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.672382579216278\n",
      "MSE: 17.18164031825407\n",
      "RMSE: 4.145074223491549\n",
      "MAE = 3.67 (%)\n",
      "MAPE = 300.48 (%)\n",
      "RSME = 4.15 (%)\n"
     ]
    }
   ],
   "source": [
    "#SARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Fit the SARIMA model\n",
    "model = SARIMAX(train_data['nat_demand'], order=(5, 1, 0), seasonal_order=(0, 1, 1, 24))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model_fit.predict(start=len(train_data), end=len(data)-1)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(test_data['nat_demand'], predictions)\n",
    "mse = mean_squared_error(test_data['nat_demand'], predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('MAE:', mae)\n",
    "print('MSE:', mse)\n",
    "print('RMSE:', rmse)\n",
    "ypred=predictions\n",
    "y_test=test_data['nat_demand']\n",
    "\n",
    "#For MAE\n",
    "mae=np.mean(np.abs(ypred-y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "def mean_absolute_percentage_error(y_test, ypred): \n",
    "    return np.mean(np.abs((y_test - ypred)/y_test))*100.\n",
    "mape = mean_absolute_percentage_error(y_test, ypred)\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bc81247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "547/547 [==============================] - 10s 7ms/step - loss: 0.0405 - val_loss: 0.0047\n",
      "Epoch 2/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 3/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 4/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 5/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 6/50\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 7/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 8/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 9/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 10/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 11/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 12/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 13/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 14/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 15/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 16/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 17/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 18/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 19/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 20/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 21/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 22/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 9.9389e-04 - val_loss: 0.0014\n",
      "Epoch 23/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 24/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 9.5888e-04 - val_loss: 9.5757e-04\n",
      "Epoch 25/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 9.4820e-04 - val_loss: 9.6264e-04\n",
      "Epoch 26/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 9.2346e-04 - val_loss: 0.0014\n",
      "Epoch 27/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 9.6316e-04 - val_loss: 0.0012\n",
      "Epoch 28/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 9.2255e-04 - val_loss: 8.9794e-04\n",
      "Epoch 29/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 8.9176e-04 - val_loss: 0.0011\n",
      "Epoch 30/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 9.0218e-04 - val_loss: 8.9484e-04\n",
      "Epoch 31/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 8.7481e-04 - val_loss: 9.7305e-04\n",
      "Epoch 32/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 8.6006e-04 - val_loss: 8.6893e-04\n",
      "Epoch 33/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 8.5785e-04 - val_loss: 0.0011\n",
      "Epoch 34/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 8.4623e-04 - val_loss: 8.2729e-04\n",
      "Epoch 35/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 8.2663e-04 - val_loss: 8.7477e-04\n",
      "Epoch 36/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 8.2533e-04 - val_loss: 9.3186e-04\n",
      "Epoch 37/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 7.9408e-04 - val_loss: 0.0016\n",
      "Epoch 38/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 8.2292e-04 - val_loss: 0.0011\n",
      "Epoch 39/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 7.6466e-04 - val_loss: 8.9584e-04\n",
      "Epoch 40/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 7.6268e-04 - val_loss: 8.0406e-04\n",
      "Epoch 41/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 7.8348e-04 - val_loss: 7.7990e-04\n",
      "Epoch 42/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 7.3687e-04 - val_loss: 7.6869e-04\n",
      "Epoch 43/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 7.5369e-04 - val_loss: 9.1615e-04\n",
      "Epoch 44/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 7.2424e-04 - val_loss: 0.0013\n",
      "Epoch 45/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 7.3519e-04 - val_loss: 0.0011\n",
      "Epoch 46/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 7.2988e-04 - val_loss: 8.1969e-04\n",
      "Epoch 47/50\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 6.8090e-04 - val_loss: 9.0902e-04\n",
      "Epoch 48/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 7.1590e-04 - val_loss: 7.5525e-04\n",
      "Epoch 49/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 6.8112e-04 - val_loss: 8.1349e-04\n",
      "Epoch 50/50\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 6.6748e-04 - val_loss: 9.6775e-04\n",
      "274/274 [==============================] - 1s 2ms/step\n",
      "274/274 [==============================] - 1s 2ms/step\n",
      "MAE = 0.02 (%)\n",
      "MAPE = 1.92 (%)\n",
      "RSME = 0.03 (%)\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data for LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1, X_train.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "mae = history.history['val_loss'][-1]\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "ypred=model.predict(X_test)\n",
    "ypred = ypred.flatten() \n",
    "\n",
    "#For MAE\n",
    "mae=np.mean(np.abs(ypred-y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "def mean_absolute_percentage_error(y_test, ypred): \n",
    "    return np.mean(np.abs((y_test - ypred)/y_test))*100.\n",
    "mape = mean_absolute_percentage_error(y_test, ypred)\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05348e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "547/547 [==============================] - 12s 18ms/step - loss: 0.0255 - val_loss: 0.0028\n",
      "Epoch 2/50\n",
      "547/547 [==============================] - 11s 19ms/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 3/50\n",
      "547/547 [==============================] - 12s 21ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 4/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 0.0016 - val_loss: 0.0034\n",
      "Epoch 5/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 6/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 7/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 8/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 9/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 10/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 11/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 12/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 13/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 14/50\n",
      "547/547 [==============================] - 12s 21ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 15/50\n",
      "547/547 [==============================] - 11s 21ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 16/50\n",
      "547/547 [==============================] - 12s 21ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 17/50\n",
      "547/547 [==============================] - 12s 21ms/step - loss: 9.9809e-04 - val_loss: 0.0010\n",
      "Epoch 18/50\n",
      "547/547 [==============================] - 11s 20ms/step - loss: 9.9026e-04 - val_loss: 0.0010\n",
      "Epoch 19/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 9.3638e-04 - val_loss: 0.0010\n",
      "Epoch 20/50\n",
      "547/547 [==============================] - 10s 19ms/step - loss: 8.5834e-04 - val_loss: 0.0012\n",
      "Epoch 21/50\n",
      "547/547 [==============================] - 10s 19ms/step - loss: 8.0803e-04 - val_loss: 9.2963e-04\n",
      "Epoch 22/50\n",
      "547/547 [==============================] - 11s 20ms/step - loss: 7.9158e-04 - val_loss: 8.7463e-04\n",
      "Epoch 23/50\n",
      "547/547 [==============================] - 10s 19ms/step - loss: 7.3420e-04 - val_loss: 8.2427e-04\n",
      "Epoch 24/50\n",
      "547/547 [==============================] - 11s 19ms/step - loss: 7.1484e-04 - val_loss: 8.0765e-04\n",
      "Epoch 25/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 6.9027e-04 - val_loss: 8.8699e-04\n",
      "Epoch 26/50\n",
      "547/547 [==============================] - 10s 17ms/step - loss: 6.7416e-04 - val_loss: 8.0049e-04\n",
      "Epoch 27/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 6.4591e-04 - val_loss: 9.0198e-04\n",
      "Epoch 28/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 6.3108e-04 - val_loss: 8.6637e-04\n",
      "Epoch 29/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 6.3551e-04 - val_loss: 7.7217e-04\n",
      "Epoch 30/50\n",
      "547/547 [==============================] - 10s 19ms/step - loss: 6.4632e-04 - val_loss: 7.4503e-04\n",
      "Epoch 31/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 5.9839e-04 - val_loss: 8.4288e-04\n",
      "Epoch 32/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 6.0701e-04 - val_loss: 7.2890e-04\n",
      "Epoch 33/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 5.8615e-04 - val_loss: 7.4522e-04\n",
      "Epoch 34/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 5.8089e-04 - val_loss: 6.8570e-04\n",
      "Epoch 35/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.6245e-04 - val_loss: 6.9819e-04\n",
      "Epoch 36/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.6176e-04 - val_loss: 6.4866e-04\n",
      "Epoch 37/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.5637e-04 - val_loss: 7.2757e-04\n",
      "Epoch 38/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.5767e-04 - val_loss: 6.4592e-04\n",
      "Epoch 39/50\n",
      "547/547 [==============================] - 10s 17ms/step - loss: 5.3721e-04 - val_loss: 7.3492e-04\n",
      "Epoch 40/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.5349e-04 - val_loss: 7.3351e-04\n",
      "Epoch 41/50\n",
      "547/547 [==============================] - 10s 19ms/step - loss: 5.4886e-04 - val_loss: 0.0015\n",
      "Epoch 42/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.3703e-04 - val_loss: 6.3305e-04\n",
      "Epoch 43/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 5.2690e-04 - val_loss: 6.7469e-04\n",
      "Epoch 44/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 5.3497e-04 - val_loss: 7.6351e-04\n",
      "Epoch 45/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 5.1709e-04 - val_loss: 7.9142e-04\n",
      "Epoch 46/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 5.2117e-04 - val_loss: 6.7149e-04\n",
      "Epoch 47/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 5.1402e-04 - val_loss: 7.5284e-04\n",
      "Epoch 48/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 5.2970e-04 - val_loss: 7.0956e-04\n",
      "Epoch 49/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.0834e-04 - val_loss: 7.0592e-04\n",
      "Epoch 50/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.0856e-04 - val_loss: 6.3995e-04\n",
      "274/274 [==============================] - 2s 5ms/step\n",
      "274/274 [==============================] - 1s 5ms/step\n",
      "MAE = 0.02 (%)\n",
      "MAPE = 1.48 (%)\n",
      "RSME = 0.03 (%)\n"
     ]
    }
   ],
   "source": [
    "#CNN-LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "mae = history.history['val_loss'][-1]\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "ypred=model.predict(X_test)\n",
    "\n",
    "#For MAE\n",
    "mae = np.mean(np.abs(ypred.ravel() - y_test.ravel()))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "mape = np.mean(np.abs((y_test.ravel() - ypred.ravel()) / y_test.ravel())) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faf85de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "547/547 [==============================] - 15s 23ms/step - loss: 0.0339 - val_loss: 0.0042\n",
      "Epoch 2/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0049 - val_loss: 0.0027\n",
      "Epoch 3/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 4/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0032 - val_loss: 0.0022\n",
      "Epoch 5/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 6/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 7/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 8/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 9/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 10/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 11/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 12/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 13/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 14/50\n",
      "547/547 [==============================] - 13s 23ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 15/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 16/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 17/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 18/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 19/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 20/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 21/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 22/50\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 23/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 24/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 25/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 26/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 27/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 28/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 29/50\n",
      "547/547 [==============================] - 12s 22ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 30/50\n",
      "547/547 [==============================] - 11s 21ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 31/50\n",
      "547/547 [==============================] - 13s 23ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 32/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 33/50\n",
      "547/547 [==============================] - 12s 23ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 34/50\n",
      "547/547 [==============================] - 13s 23ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 35/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 36/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 37/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 38/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 39/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 40/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 41/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 42/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 43/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 44/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 45/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 9.8114e-04 - val_loss: 9.7990e-04\n",
      "Epoch 46/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 9.4739e-04 - val_loss: 0.0012\n",
      "Epoch 47/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 9.2996e-04 - val_loss: 9.7999e-04\n",
      "Epoch 48/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 9.0292e-04 - val_loss: 0.0011\n",
      "Epoch 49/50\n",
      "547/547 [==============================] - 14s 25ms/step - loss: 8.8967e-04 - val_loss: 0.0011\n",
      "Epoch 50/50\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 8.8429e-04 - val_loss: 9.6127e-04\n",
      "274/274 [==============================] - 2s 6ms/step\n",
      "274/274 [==============================] - 2s 7ms/step\n",
      "MAE = 0.02 (%)\n",
      "MAPE = 1.87 (%)\n",
      "RSME = 0.03 (%)\n"
     ]
    }
   ],
   "source": [
    "#Attention\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data \n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "# Define the model\n",
    "inputs = Input(shape=input_shape)\n",
    "conv1 = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "lstm1 = LSTM(64, return_sequences=True)(pool1)\n",
    "attention = Attention()([lstm1, lstm1])\n",
    "flatten = Flatten()(attention)\n",
    "dropout = Dropout(0.2)(flatten)\n",
    "outputs = Dense(1)(dropout)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer=Adam())\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "mae = history.history['val_loss'][-1]\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "ypred = model.predict(X_test)\n",
    "\n",
    "#For MAE\n",
    "mae = np.mean(np.abs(ypred.ravel() - y_test.ravel()))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "mape = np.mean(np.abs((y_test.ravel() - ypred.ravel()) / y_test.ravel())) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96f9c2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "547/547 [==============================] - 68s 118ms/step - loss: 0.8959 - val_loss: 0.6115\n",
      "Epoch 2/50\n",
      "547/547 [==============================] - 60s 109ms/step - loss: 0.2959 - val_loss: 0.2049\n",
      "Epoch 3/50\n",
      "547/547 [==============================] - 57s 105ms/step - loss: 0.0894 - val_loss: 0.0768\n",
      "Epoch 4/50\n",
      "547/547 [==============================] - 57s 105ms/step - loss: 0.0429 - val_loss: 0.0475\n",
      "Epoch 5/50\n",
      "547/547 [==============================] - 58s 105ms/step - loss: 0.0371 - val_loss: 0.0421\n",
      "Epoch 6/50\n",
      "547/547 [==============================] - 58s 105ms/step - loss: 0.0368 - val_loss: 0.0413\n",
      "Epoch 7/50\n",
      "547/547 [==============================] - 58s 106ms/step - loss: 0.0368 - val_loss: 0.0411\n",
      "Epoch 8/50\n",
      "547/547 [==============================] - 57s 105ms/step - loss: 0.0368 - val_loss: 0.0414\n",
      "Epoch 9/50\n",
      "547/547 [==============================] - 57s 104ms/step - loss: 0.0368 - val_loss: 0.0413\n",
      "Epoch 10/50\n",
      "547/547 [==============================] - 57s 103ms/step - loss: 0.0368 - val_loss: 0.0409\n",
      "Epoch 11/50\n",
      "547/547 [==============================] - 57s 104ms/step - loss: 0.0368 - val_loss: 0.0417\n",
      "Epoch 12/50\n",
      "547/547 [==============================] - 57s 104ms/step - loss: 0.0368 - val_loss: 0.0407\n",
      "Epoch 13/50\n",
      "547/547 [==============================] - 58s 105ms/step - loss: 0.0368 - val_loss: 0.0421\n",
      "Epoch 14/50\n",
      "547/547 [==============================] - 59s 109ms/step - loss: 0.0368 - val_loss: 0.0411\n",
      "Epoch 15/50\n",
      "547/547 [==============================] - 59s 107ms/step - loss: 0.0368 - val_loss: 0.0423\n",
      "Epoch 16/50\n",
      "547/547 [==============================] - 59s 107ms/step - loss: 0.0368 - val_loss: 0.0417\n",
      "Epoch 17/50\n",
      "547/547 [==============================] - 58s 106ms/step - loss: 0.0368 - val_loss: 0.0409\n",
      "274/274 [==============================] - 6s 23ms/step\n",
      "274/274 [==============================] - 6s 23ms/step\n",
      "MAE = 0.17 (%)\n",
      "MAPE = 13.14 (%)\n",
      "RSME = 0.20 (%)\n"
     ]
    }
   ],
   "source": [
    "#Transformer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Define the input shape\n",
    "seq_len = X_train.shape[1]\n",
    "n_features = 1\n",
    "\n",
    "# Reshape the data for Transformer\n",
    "X_train = X_train.values.reshape(X_train.shape[0], seq_len, n_features)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], seq_len, n_features)\n",
    "\n",
    "# Define the Transformer model\n",
    "inputs = Input(shape=(seq_len, n_features))\n",
    "x = inputs\n",
    "\n",
    "# Add self-attention layers\n",
    "for _ in range(2):\n",
    "    x = tf.keras.layers.MultiHeadAttention(num_heads=8, key_dim=32)(x, x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "# Flatten and add output layer\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "outputs = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[EarlyStopping(patience=5, restore_best_weights=True)])\n",
    "\n",
    "# Evaluate the model\n",
    "mae = history.history['val_loss'][-1]\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "ypred = model.predict(X_test)\n",
    "#For MAE\n",
    "mae = np.mean(np.abs(ypred.ravel() - y_test.ravel()))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "mape = np.mean(np.abs((y_test.ravel() - ypred.ravel()) / y_test.ravel())) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c6db270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.0130\n",
      "Epoch 2/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 0.0011\n",
      "Epoch 3/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 9.1178e-04\n",
      "Epoch 4/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 8.0493e-04\n",
      "Epoch 5/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 7.4452e-04\n",
      "Epoch 6/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 7.1518e-04\n",
      "Epoch 7/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 7.2373e-04\n",
      "Epoch 8/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 6.7233e-04\n",
      "Epoch 9/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 6.6712e-04\n",
      "Epoch 10/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 6.3691e-04\n",
      "Epoch 11/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 6.2104e-04\n",
      "Epoch 12/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 6.1861e-04\n",
      "Epoch 13/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 6.1110e-04\n",
      "Epoch 14/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.9868e-04\n",
      "Epoch 15/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.8687e-04\n",
      "Epoch 16/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.6957e-04\n",
      "Epoch 17/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.6987e-04\n",
      "Epoch 18/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.5961e-04\n",
      "Epoch 19/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.5322e-04\n",
      "Epoch 20/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.4073e-04\n",
      "Epoch 21/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.7722e-04\n",
      "Epoch 22/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.6634e-04\n",
      "Epoch 23/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.3716e-04\n",
      "Epoch 24/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.4677e-04\n",
      "Epoch 25/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.3109e-04\n",
      "Epoch 26/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.3646e-04\n",
      "Epoch 27/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.4003e-04\n",
      "Epoch 28/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.2421e-04\n",
      "Epoch 29/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.4716e-04\n",
      "Epoch 30/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.2257e-04\n",
      "Epoch 31/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.1703e-04\n",
      "Epoch 32/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.1553e-04\n",
      "Epoch 33/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.2659e-04\n",
      "Epoch 34/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.0893e-04\n",
      "Epoch 35/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.0460e-04\n",
      "Epoch 36/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.1773e-04\n",
      "Epoch 37/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.0339e-04\n",
      "Epoch 38/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.1337e-04\n",
      "Epoch 39/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 4.9797e-04\n",
      "Epoch 40/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.0974e-04\n",
      "Epoch 41/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 4.9565e-04\n",
      "Epoch 42/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 4.9872e-04\n",
      "Epoch 43/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 4.9060e-04\n",
      "Epoch 44/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.0245e-04\n",
      "Epoch 45/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 4.7734e-04\n",
      "Epoch 46/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 4.8336e-04\n",
      "Epoch 47/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 5.0324e-04\n",
      "Epoch 48/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 4.8049e-04\n",
      "Epoch 49/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 4.8026e-04\n",
      "Epoch 50/50\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 4.8072e-04\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 6.2590e-04\n",
      "Test loss: 0.0006259004003368318\n",
      "274/274 [==============================] - 0s 1ms/step\n",
      "Predictions: [[1.0366418]\n",
      " [1.0388445]\n",
      " [1.0459939]\n",
      " ...\n",
      " [1.1489925]\n",
      " [1.0901428]\n",
      " [1.0513817]]\n",
      "MAE = 0.02 (%)\n",
      "MAPE = 1.46 (%)\n",
      "RMSE = 0.03 (%)\n"
     ]
    }
   ],
   "source": [
    "#CNN LeNet\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build the LeNet model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(6, kernel_size=5, activation='relu',  input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Conv1D(16, kernel_size=5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(120, activation='relu'))\n",
    "model.add(layers.Dense(84, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='linear'))  # Output layer with linear activation for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "# Make predictions on new data\n",
    "predictions = model.predict(X_test)\n",
    "ypred=predictions\n",
    "print('Predictions:', predictions)\n",
    "ypred = ypred.flatten()\n",
    "\n",
    "\n",
    "#For MAE\n",
    "mae = np.mean(np.abs(ypred - y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "mape = np.mean(np.abs((y_test - ypred) / y_test)) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "mse = mean_squared_error(y_test, ypred) \n",
    "rmse = np.sqrt(mse) \n",
    "print('RMSE = {:.2f} (%)'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0511732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 0.0062\n",
      "Epoch 2/50\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.0012\n",
      "Epoch 3/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 0.0010\n",
      "Epoch 4/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 9.1120e-04\n",
      "Epoch 5/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 8.4622e-04\n",
      "Epoch 6/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 7.9184e-04\n",
      "Epoch 7/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 7.2472e-04\n",
      "Epoch 8/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 7.2905e-04\n",
      "Epoch 9/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 7.0310e-04\n",
      "Epoch 10/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 6.6544e-04\n",
      "Epoch 11/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 6.4472e-04\n",
      "Epoch 12/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 6.2483e-04\n",
      "Epoch 13/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 6.1900e-04\n",
      "Epoch 14/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 6.0534e-04\n",
      "Epoch 15/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 6.0685e-04\n",
      "Epoch 16/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.7939e-04\n",
      "Epoch 17/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.6748e-04\n",
      "Epoch 18/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.7910e-04\n",
      "Epoch 19/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.5767e-04\n",
      "Epoch 20/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.5887e-04\n",
      "Epoch 21/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.4706e-04\n",
      "Epoch 22/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.4852e-04\n",
      "Epoch 23/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.5204e-04\n",
      "Epoch 24/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.4934e-04\n",
      "Epoch 25/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.4234e-04\n",
      "Epoch 26/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.3396e-04\n",
      "Epoch 27/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.1501e-04\n",
      "Epoch 28/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.2602e-04\n",
      "Epoch 29/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.1527e-04\n",
      "Epoch 30/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.0847e-04\n",
      "Epoch 31/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.0918e-04\n",
      "Epoch 32/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.9611e-04\n",
      "Epoch 33/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.0739e-04\n",
      "Epoch 34/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.0285e-04\n",
      "Epoch 35/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 5.0697e-04\n",
      "Epoch 36/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.9694e-04\n",
      "Epoch 37/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.8612e-04\n",
      "Epoch 38/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.9467e-04\n",
      "Epoch 39/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.8264e-04\n",
      "Epoch 40/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.9687e-04\n",
      "Epoch 41/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.7980e-04\n",
      "Epoch 42/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.8686e-04\n",
      "Epoch 43/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.8730e-04\n",
      "Epoch 44/50\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 4.7726e-04\n",
      "Epoch 45/50\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 4.8059e-04\n",
      "Epoch 46/50\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 4.8162e-04\n",
      "Epoch 47/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.7255e-04\n",
      "Epoch 48/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.6769e-04\n",
      "Epoch 49/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.7386e-04\n",
      "Epoch 50/50\n",
      "1094/1094 [==============================] - 3s 2ms/step - loss: 4.7179e-04\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 9.3349e-04\n",
      "Test loss: 0.0009334909846074879\n",
      "274/274 [==============================] - 0s 1ms/step\n",
      "Predictions: [[1.0368878]\n",
      " [1.0316536]\n",
      " [1.0360491]\n",
      " ...\n",
      " [1.1200709]\n",
      " [1.0584955]\n",
      " [1.0309415]]\n",
      "MAE = 0.02 (%)\n",
      "MAPE = 1.88 (%)\n",
      "RMSE = 0.03 (%)\n"
     ]
    }
   ],
   "source": [
    "#CNN LeNet\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build the LeNet model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(6, kernel_size=5, activation='relu',  input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Conv1D(16, kernel_size=5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(120, activation='relu'))\n",
    "model.add(layers.Dense(84, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='linear'))  # Output layer with linear activation for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "# Make predictions on new data\n",
    "predictions = model.predict(X_test)\n",
    "ypred=predictions\n",
    "print('Predictions:', predictions)\n",
    "ypred = ypred.flatten()\n",
    "\n",
    "\n",
    "#For MAE\n",
    "mae = np.mean(np.abs(ypred - y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "mape = np.mean(np.abs((y_test - ypred) / y_test)) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "mse = mean_squared_error(y_test, ypred) \n",
    "rmse = np.sqrt(mse) \n",
    "print('RMSE = {:.2f} (%)'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c18ba749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1094/1094 [==============================] - 10s 7ms/step - loss: 0.0175\n",
      "Epoch 2/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 0.0013\n",
      "Epoch 3/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 0.0011\n",
      "Epoch 4/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 0.0010\n",
      "Epoch 5/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 8.7078e-04\n",
      "Epoch 6/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 8.0318e-04\n",
      "Epoch 7/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 7.8339e-04\n",
      "Epoch 8/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 7.5102e-04\n",
      "Epoch 9/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 7.0346e-04\n",
      "Epoch 10/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 7.0935e-04\n",
      "Epoch 11/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 6.7021e-04\n",
      "Epoch 12/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 6.8334e-04\n",
      "Epoch 13/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 6.8162e-04\n",
      "Epoch 14/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 6.2694e-04\n",
      "Epoch 15/50\n",
      "1094/1094 [==============================] - 8s 8ms/step - loss: 6.0531e-04\n",
      "Epoch 16/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 6.2755e-04\n",
      "Epoch 17/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.9844e-04\n",
      "Epoch 18/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 6.0920e-04\n",
      "Epoch 19/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.8134e-04\n",
      "Epoch 20/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.9363e-04\n",
      "Epoch 21/50\n",
      "1094/1094 [==============================] - 8s 8ms/step - loss: 5.8788e-04\n",
      "Epoch 22/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.4956e-04\n",
      "Epoch 23/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.8097e-04\n",
      "Epoch 24/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.8303e-04\n",
      "Epoch 25/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.4563e-04\n",
      "Epoch 26/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.4765e-04\n",
      "Epoch 27/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.4751e-04\n",
      "Epoch 28/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.3805e-04\n",
      "Epoch 29/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.2733e-04\n",
      "Epoch 30/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.4759e-04\n",
      "Epoch 31/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.2490e-04\n",
      "Epoch 32/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.2264e-04\n",
      "Epoch 33/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.2837e-04\n",
      "Epoch 34/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.3900e-04\n",
      "Epoch 35/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.0623e-04\n",
      "Epoch 36/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.1726e-04\n",
      "Epoch 37/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.2286e-04\n",
      "Epoch 38/50\n",
      "1094/1094 [==============================] - 8s 8ms/step - loss: 5.1641e-04\n",
      "Epoch 39/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.0936e-04\n",
      "Epoch 40/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 4.9777e-04\n",
      "Epoch 41/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.0357e-04\n",
      "Epoch 42/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 5.0830e-04\n",
      "Epoch 43/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 4.8607e-04\n",
      "Epoch 44/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 4.8357e-04\n",
      "Epoch 45/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 4.8123e-04\n",
      "Epoch 46/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 4.7871e-04\n",
      "Epoch 47/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 4.9897e-04\n",
      "Epoch 48/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 4.8455e-04\n",
      "Epoch 49/50\n",
      "1094/1094 [==============================] - 8s 8ms/step - loss: 4.7971e-04\n",
      "Epoch 50/50\n",
      "1094/1094 [==============================] - 8s 7ms/step - loss: 4.8668e-04\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 6.8236e-04\n",
      "Test loss: 0.0006823648000136018\n",
      "274/274 [==============================] - 1s 3ms/step\n",
      "MAE = 0.02 (%)\n",
      "MAPE = 1.48 (%)\n",
      "RMSE = 0.03 (%)\n"
     ]
    }
   ],
   "source": [
    "#LeNet-LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build the LeNet model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(6, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Conv1D(16, kernel_size=5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(120, activation='relu'))\n",
    "model.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Add LSTM layer\n",
    "model.add(layers.Reshape((7, 12)))  # Reshape the output of dense layers for compatibility with LSTM\n",
    "model.add(layers.LSTM(84, return_sequences=False))\n",
    "\n",
    "model.add(layers.Dense(1, activation='linear'))  # Output layer with linear activation for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "# Make predictions on new data\n",
    "predictions = model.predict(X_test)\n",
    "ypred = predictions.flatten()\n",
    "\n",
    "# For MAE\n",
    "mae = np.mean(np.abs(ypred - y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "# For MAPE\n",
    "mape = np.mean(np.abs((y_test - ypred) / y_test)) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "mse = mean_squared_error(y_test, ypred) \n",
    "rmse = np.sqrt(mse) \n",
    "print('RMSE = {:.2f} (%)'.format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f78c4648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1094/1094 [==============================] - 15s 11ms/step - loss: 0.0143\n",
      "Epoch 2/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 0.0013\n",
      "Epoch 3/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 0.0010\n",
      "Epoch 4/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 8.9521e-04\n",
      "Epoch 5/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 8.8741e-04\n",
      "Epoch 6/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 8.2278e-04\n",
      "Epoch 7/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 7.2968e-04\n",
      "Epoch 8/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 7.5500e-04\n",
      "Epoch 9/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 7.1939e-04\n",
      "Epoch 10/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 7.3628e-04\n",
      "Epoch 11/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 6.6153e-04\n",
      "Epoch 12/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 6.6949e-04\n",
      "Epoch 13/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 6.7270e-04\n",
      "Epoch 14/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 6.8667e-04\n",
      "Epoch 15/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 6.3714e-04\n",
      "Epoch 16/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 6.6306e-04\n",
      "Epoch 17/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 6.3666e-04\n",
      "Epoch 18/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 6.2712e-04\n",
      "Epoch 19/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.9688e-04\n",
      "Epoch 20/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 6.0437e-04\n",
      "Epoch 21/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.7525e-04\n",
      "Epoch 22/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.7495e-04\n",
      "Epoch 23/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 6.0082e-04\n",
      "Epoch 24/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.7752e-04\n",
      "Epoch 25/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 6.0621e-04\n",
      "Epoch 26/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.7871e-04\n",
      "Epoch 27/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.5913e-04\n",
      "Epoch 28/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.6202e-04\n",
      "Epoch 29/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.8646e-04\n",
      "Epoch 30/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.4871e-04\n",
      "Epoch 31/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.6173e-04\n",
      "Epoch 32/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 5.3893e-04\n",
      "Epoch 33/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.5010e-04\n",
      "Epoch 34/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.4522e-04\n",
      "Epoch 35/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.4307e-04\n",
      "Epoch 36/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.3884e-04\n",
      "Epoch 37/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.1791e-04\n",
      "Epoch 38/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.2356e-04\n",
      "Epoch 39/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.3162e-04\n",
      "Epoch 40/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.3427e-04\n",
      "Epoch 41/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.1985e-04\n",
      "Epoch 42/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.2418e-04\n",
      "Epoch 43/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.1045e-04\n",
      "Epoch 44/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 5.0976e-04\n",
      "Epoch 45/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 5.2197e-04\n",
      "Epoch 46/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.0936e-04\n",
      "Epoch 47/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.0860e-04\n",
      "Epoch 48/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 5.0423e-04\n",
      "Epoch 49/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 5.1283e-04\n",
      "Epoch 50/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.1297e-04\n",
      "Epoch 51/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.9801e-04\n",
      "Epoch 52/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 4.9491e-04\n",
      "Epoch 53/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 5.0460e-04\n",
      "Epoch 54/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 5.0702e-04\n",
      "Epoch 55/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 4.9780e-04\n",
      "Epoch 56/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 5.0569e-04\n",
      "Epoch 57/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.8383e-04\n",
      "Epoch 58/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 5.0336e-04\n",
      "Epoch 59/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 4.8295e-04\n",
      "Epoch 60/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 4.9467e-04\n",
      "Epoch 61/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.8702e-04\n",
      "Epoch 62/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 4.9135e-04\n",
      "Epoch 63/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 4.8359e-04\n",
      "Epoch 64/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 4.9248e-04\n",
      "Epoch 65/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.8164e-04\n",
      "Epoch 66/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.7714e-04\n",
      "Epoch 67/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.7927e-04\n",
      "Epoch 68/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.8321e-04\n",
      "Epoch 69/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 4.6704e-04\n",
      "Epoch 70/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 4.6852e-04\n",
      "Epoch 71/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 4.7253e-04\n",
      "Epoch 72/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 4.7866e-04\n",
      "Epoch 73/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 4.7134e-04\n",
      "Epoch 74/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.7431e-04\n",
      "Epoch 75/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 4.6794e-04\n",
      "Epoch 76/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 4.7438e-04\n",
      "Epoch 77/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.6683e-04\n",
      "Epoch 78/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.7626e-04\n",
      "Epoch 79/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.7433e-04\n",
      "Epoch 80/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 4.6633e-04\n",
      "Epoch 81/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 4.5523e-04\n",
      "Epoch 82/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 4.7553e-04\n",
      "Epoch 83/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.7172e-04\n",
      "Epoch 84/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.7001e-04\n",
      "Epoch 85/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 4.5551e-04\n",
      "Epoch 86/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.5671e-04\n",
      "Epoch 87/100\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 4.6733e-04\n",
      "Epoch 88/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.6261e-04\n",
      "Epoch 89/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.6281e-04\n",
      "Epoch 90/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.5970e-04\n",
      "Epoch 91/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.5348e-04\n",
      "Epoch 92/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.6009e-04\n",
      "Epoch 93/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.5059e-04\n",
      "Epoch 94/100\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 4.6363e-04\n",
      "Epoch 95/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.4329e-04\n",
      "Epoch 96/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.5700e-04\n",
      "Epoch 97/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.6897e-04\n",
      "Epoch 98/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.5910e-04\n",
      "Epoch 99/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.5495e-04\n",
      "Epoch 100/100\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 4.4588e-04\n",
      "274/274 [==============================] - 2s 4ms/step - loss: 6.0392e-04\n",
      "Test loss: 0.0006039203144609928\n",
      "274/274 [==============================] - 2s 4ms/step\n",
      "MAE = 0.02 (%)\n",
      "MAPE = 1.39 (%)\n",
      "RMSE = 0.02 (%)\n"
     ]
    }
   ],
   "source": [
    "#GoogleNet Lstm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build the GoogleNet-LSTM model\n",
    "def inception_module(x, filters):\n",
    "    branch1x1 = layers.Conv1D(filters[0], 1, activation='relu', padding='same')(x)\n",
    "    \n",
    "    branch3x3 = layers.Conv1D(filters[1], 1, activation='relu', padding='same')(x)\n",
    "    branch3x3 = layers.Conv1D(filters[2], 3, activation='relu', padding='same')(branch3x3)\n",
    "    \n",
    "    branch5x5 = layers.Conv1D(filters[3], 1, activation='relu', padding='same')(x)\n",
    "    branch5x5 = layers.Conv1D(filters[4], 5, activation='relu', padding='same')(branch5x5)\n",
    "    \n",
    "    branch_pool = layers.MaxPooling1D(3, strides=1, padding='same')(x)\n",
    "    branch_pool = layers.Conv1D(filters[5], 1, activation='relu', padding='same')(branch_pool)\n",
    "    \n",
    "    output = layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1)\n",
    "    return output\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # Adjust input shape as per your requirements\n",
    "\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "x = layers.Conv1D(6, kernel_size=5, activation='relu', padding='same')(input_layer)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "x = layers.Conv1D(16, kernel_size=5, activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "# Replace fully connected layers with Inception modules\n",
    "x = inception_module(x, [6, 6, 8, 2, 4, 4])\n",
    "x = inception_module(x, [16, 8, 12, 4, 8, 8])\n",
    "\n",
    "x = layers.LSTM(84, return_sequences=True)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = models.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "ypred = predictions.flatten()\n",
    "\n",
    "# For MAE\n",
    "mae = np.mean(np.abs(ypred - y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "# For MAPE\n",
    "mape = np.mean(np.abs((y_test - ypred) / y_test)) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "mse = mean_squared_error(y_test, ypred) \n",
    "rmse = np.sqrt(mse) \n",
    "print('RMSE = {:.2f} (%)'.format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0bc1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "547/547 [==============================] - 11s 17ms/step - loss: 0.0210 - val_loss: 0.0029\n",
      "Epoch 2/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 3/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 4/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 5/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 6/50\n",
      "547/547 [==============================] - 8s 15ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 7/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 8/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 9/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 10/50\n",
      "547/547 [==============================] - 8s 15ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 11/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 12/50\n",
      "547/547 [==============================] - 10s 19ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 13/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 14/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 15/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 16/50\n",
      "547/547 [==============================] - 8s 15ms/step - loss: 9.7915e-04 - val_loss: 0.0012\n",
      "Epoch 17/50\n",
      "547/547 [==============================] - 8s 15ms/step - loss: 9.4250e-04 - val_loss: 0.0010\n",
      "Epoch 18/50\n",
      "547/547 [==============================] - 8s 15ms/step - loss: 9.1711e-04 - val_loss: 0.0012\n",
      "Epoch 19/50\n",
      "547/547 [==============================] - 8s 16ms/step - loss: 8.9198e-04 - val_loss: 0.0011\n",
      "Epoch 20/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 8.6025e-04 - val_loss: 0.0010\n",
      "Epoch 21/50\n",
      "547/547 [==============================] - 8s 15ms/step - loss: 8.2172e-04 - val_loss: 0.0010\n",
      "Epoch 22/50\n",
      "547/547 [==============================] - 8s 16ms/step - loss: 8.1475e-04 - val_loss: 9.0793e-04\n",
      "Epoch 23/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 7.7341e-04 - val_loss: 8.8314e-04\n",
      "Epoch 24/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 7.3920e-04 - val_loss: 8.2396e-04\n",
      "Epoch 25/50\n",
      "547/547 [==============================] - 8s 15ms/step - loss: 7.4523e-04 - val_loss: 8.6338e-04\n",
      "Epoch 26/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 7.2695e-04 - val_loss: 8.5981e-04\n",
      "Epoch 27/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 7.0380e-04 - val_loss: 8.3859e-04\n",
      "Epoch 28/50\n",
      "547/547 [==============================] - 10s 17ms/step - loss: 7.0936e-04 - val_loss: 8.1296e-04\n",
      "Epoch 29/50\n",
      "547/547 [==============================] - 10s 17ms/step - loss: 6.7596e-04 - val_loss: 8.1017e-04\n",
      "Epoch 30/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 6.7380e-04 - val_loss: 8.2146e-04\n",
      "Epoch 31/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 6.3976e-04 - val_loss: 7.8400e-04\n",
      "Epoch 32/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 6.7319e-04 - val_loss: 8.2974e-04\n",
      "Epoch 33/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 6.4614e-04 - val_loss: 9.3150e-04\n",
      "Epoch 34/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 6.3469e-04 - val_loss: 7.5381e-04\n",
      "Epoch 35/50\n",
      "547/547 [==============================] - 10s 18ms/step - loss: 6.0955e-04 - val_loss: 7.1363e-04\n",
      "Epoch 36/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 6.2079e-04 - val_loss: 7.9104e-04\n",
      "Epoch 37/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.9967e-04 - val_loss: 7.3688e-04\n",
      "Epoch 38/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.8703e-04 - val_loss: 7.0353e-04\n",
      "Epoch 39/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 5.7846e-04 - val_loss: 6.9293e-04\n",
      "Epoch 40/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.7346e-04 - val_loss: 7.0431e-04\n",
      "Epoch 41/50\n",
      "547/547 [==============================] - 9s 17ms/step - loss: 5.7240e-04 - val_loss: 7.2845e-04\n",
      "Epoch 42/50\n",
      "547/547 [==============================] - 10s 19ms/step - loss: 5.7722e-04 - val_loss: 7.1002e-04\n",
      "Epoch 43/50\n",
      "547/547 [==============================] - 10s 17ms/step - loss: 5.5358e-04 - val_loss: 7.6503e-04\n",
      "Epoch 44/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 5.6441e-04 - val_loss: 9.2959e-04\n",
      "Epoch 45/50\n",
      "547/547 [==============================] - 10s 19ms/step - loss: 5.4607e-04 - val_loss: 7.3010e-04\n",
      "Epoch 46/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 5.3334e-04 - val_loss: 6.7938e-04\n",
      "Epoch 47/50\n",
      "547/547 [==============================] - 10s 17ms/step - loss: 5.4578e-04 - val_loss: 6.4733e-04\n",
      "Epoch 48/50\n",
      "547/547 [==============================] - 9s 16ms/step - loss: 5.2692e-04 - val_loss: 6.7530e-04\n",
      "Epoch 49/50\n",
      "547/547 [==============================] - 15s 27ms/step - loss: 5.3536e-04 - val_loss: 7.8618e-04\n",
      "Epoch 50/50\n",
      "547/547 [==============================] - 10s 17ms/step - loss: 5.1789e-04 - val_loss: 7.1212e-04\n",
      "274/274 [==============================] - 2s 6ms/step\n",
      "274/274 [==============================] - 1s 5ms/step\n",
      "MAE = 0.02 (%)\n",
      "MAPE = 1.59 (%)\n",
      "RMSE = 0.03 (%)\n"
     ]
    }
   ],
   "source": [
    "#CNN-Attention\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "mae = history.history['val_loss'][-1]\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "ypred=model.predict(X_test)\n",
    "ypred = ypred.flatten()\n",
    "#For MAE\n",
    "mae = np.mean(np.abs(ypred - y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "mape = np.mean(np.abs((y_test - ypred) / y_test)) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "mse = mean_squared_error(y_test, ypred) \n",
    "rmse = np.sqrt(mse) \n",
    "print('RMSE = {:.2f} (%)'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb759fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
