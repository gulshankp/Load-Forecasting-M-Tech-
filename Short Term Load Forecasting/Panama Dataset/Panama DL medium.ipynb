{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0504fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[:35039]\n",
    "#Data Preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Load the data\n",
    "data = pd.read_csv('continuous dataset.csv')\n",
    "\n",
    "# Convert timestamp to datetime format\n",
    "data['datetime'] = data['datetime'][26256:37176].apply(lambda x: datetime.strptime(x, '%d-%m-%Y %H:%M'))\n",
    "\n",
    "# Set the datetime as index\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "# Resample the data to hourly frequency\n",
    "data = data.resample('H').mean()\n",
    "\n",
    "# Create lag features\n",
    "for i in range(1, 25):\n",
    "    data['lag_{}'.format(i)] = data['nat_demand'].shift(i)\n",
    "\n",
    "# Create rolling mean and standard deviation features\n",
    "data['rolling_mean'] = data['nat_demand'].rolling(window=24).mean()\n",
    "data['rolling_std'] = data['nat_demand'].rolling(window=24).std()\n",
    "\n",
    "# Create weekday and hour features\n",
    "data['weekday'] = data.index.weekday\n",
    "data['hour'] = data.index.hour\n",
    "\n",
    "# Remove missing values\n",
    "data.dropna(inplace=True)\n",
    "train_size = int(len(data) * 0.8)\n",
    "train_data = data[:train_size]/1000\n",
    "test_data = data[train_size:]/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e51951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gulsh\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\gulsh\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\gulsh\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7824\\1425201135.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Fit the ARIMA model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mARIMA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nat_demand'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Make predictions on the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\arima\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, start_params, transformed, includes_fixed, method, method_kwargs, gls, gls_kwargs, cov_type, cov_kwds, return_params, low_memory)\u001b[0m\n\u001b[0;32m    388\u001b[0m                 \u001b[0mmethod_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'disp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                 res = super().fit(\n\u001b[0m\u001b[0;32m    391\u001b[0m                     \u001b[0mreturn_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                     cov_type=cov_type, cov_kwds=cov_kwds, **method_kwargs)\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m                 \u001b[0mflags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hessian_method'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim_hessian\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[0mfargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m             mlefit = super(MLEModel, self).fit(start_params, method=method,\n\u001b[0m\u001b[0;32m    705\u001b[0m                                                \u001b[0mfargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                                                \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m         xopt, retvals, optim_settings = optimizer._fit(f, score, start_params,\n\u001b[0m\u001b[0;32m    564\u001b[0m                                                        \u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m                                                        \u001b[0mhessian\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\statsmodels\\base\\optimizer.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_funcs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         xopt, retvals = func(objective, gradient, start_params, fargs, kwargs,\n\u001b[0m\u001b[0;32m    242\u001b[0m                              \u001b[0mdisp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m                              \u001b[0mretall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\statsmodels\\base\\optimizer.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[1;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m     retvals = optimize.fmin_l_bfgs_b(func, start_params, maxiter=maxiter,\n\u001b[0m\u001b[0;32m    652\u001b[0m                                      \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m                                      \u001b[0mbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[0;32m    197\u001b[0m             'maxls': maxls}\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0m\u001b[0;32m    200\u001b[0m                            **opts)\n\u001b[0;32m    201\u001b[0m     d = {'grad': res['jac'],\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    360\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n\u001b[0m\u001b[0;32m    174\u001b[0m                                            **finite_diff_options)\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\scipy\\optimize\\_numdiff.py\u001b[0m in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msparsity\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m             return _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[0m\u001b[0;32m    506\u001b[0m                                      use_one_sided, method)\n\u001b[0;32m    507\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\scipy\\optimize\\_numdiff.py\u001b[0m in \u001b[0;36m_dense_difference\u001b[1;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Recompute dx as exactly representable number.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'3-point'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0muse_one_sided\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\scipy\\optimize\\_numdiff.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m             raise RuntimeError(\"`fun` return value has \"\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[1;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;31m# Make sure the function returns a true scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(params, *args)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloglike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py\u001b[0m in \u001b[0;36mloglike\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    937\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'inversion_method'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mINVERT_UNIVARIATE\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mSOLVE_LU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m         \u001b[0mloglike\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mssm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloglike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplex_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomplex_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[1;31m# Koopman, Shephard, and Doornik recommend maximizing the average\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda32\\lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_filter.py\u001b[0m in \u001b[0;36mloglike\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m         kwargs.setdefault('conserve_memory',\n\u001b[0;32m    982\u001b[0m                           MEMORY_CONSERVE ^ MEMORY_NO_LIKELIHOOD)\n\u001b[1;32m--> 983\u001b[1;33m         \u001b[0mkfilter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m         loglikelihood_burn = kwargs.get('loglikelihood_burn',\n\u001b[0;32m    985\u001b[0m                                         self.loglikelihood_burn)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit the ARIMA model\n",
    "model = ARIMA(train_data['nat_demand'], order=(5, 1, 0))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model_fit.predict(start=len(train_data), end=len(data)-1, typ='levels')\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(test_data['nat_demand'], predictions)\n",
    "mse = mean_squared_error(test_data['nat_demand'], predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('MAE:', mae)\n",
    "print('MSE:', mse)\n",
    "print('RMSE:', rmse)\n",
    "ypred=predictions\n",
    "y_test=test_data['nat_demand']\n",
    "\n",
    "#For MAE\n",
    "mae=np.mean(np.abs(ypred-y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "def mean_absolute_percentage_error(y_test, ypred): \n",
    "    return np.mean(np.abs((y_test - ypred)/y_test))*100.\n",
    "mape = mean_absolute_percentage_error(y_test, ypred)\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Fit the SARIMA model\n",
    "model = SARIMAX(train_data['nat_demand'], order=(5, 1, 0), seasonal_order=(0, 1, 1, 24))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model_fit.predict(start=len(train_data), end=len(data)-1)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(test_data['nat_demand'], predictions)\n",
    "mse = mean_squared_error(test_data['nat_demand'], predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('MAE:', mae)\n",
    "print('MSE:', mse)\n",
    "print('RMSE:', rmse)\n",
    "ypred=predictions\n",
    "y_test=test_data['nat_demand']\n",
    "\n",
    "#For MAE\n",
    "mae=np.mean(np.abs(ypred-y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "def mean_absolute_percentage_error(y_test, ypred): \n",
    "    return np.mean(np.abs((y_test - ypred)/y_test))*100.\n",
    "mape = mean_absolute_percentage_error(y_test, ypred)\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1cd9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data for LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1, X_train.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "mae = history.history['val_loss'][-1]\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "ypred=model.predict(X_test)\n",
    "ypred = ypred.flatten() \n",
    "\n",
    "#For MAE\n",
    "mae=np.mean(np.abs(ypred-y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "def mean_absolute_percentage_error(y_test, ypred): \n",
    "    return np.mean(np.abs((y_test - ypred)/y_test))*100.\n",
    "mape = mean_absolute_percentage_error(y_test, ypred)\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30697b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN-LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "mae = history.history['val_loss'][-1]\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "ypred=model.predict(X_test)\n",
    "\n",
    "#For MAE\n",
    "mae = np.mean(np.abs(ypred.ravel() - y_test.ravel()))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "mape = np.mean(np.abs((y_test.ravel() - ypred.ravel()) / y_test.ravel())) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data \n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "# Define the model\n",
    "inputs = Input(shape=input_shape)\n",
    "conv1 = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "lstm1 = LSTM(64, return_sequences=True)(pool1)\n",
    "attention = Attention()([lstm1, lstm1])\n",
    "flatten = Flatten()(attention)\n",
    "dropout = Dropout(0.2)(flatten)\n",
    "outputs = Dense(1)(dropout)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer=Adam())\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "mae = history.history['val_loss'][-1]\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "ypred = model.predict(X_test)\n",
    "\n",
    "#For MAE\n",
    "mae = np.mean(np.abs(ypred.ravel() - y_test.ravel()))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "mape = np.mean(np.abs((y_test.ravel() - ypred.ravel()) / y_test.ravel())) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c54376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Define the input shape\n",
    "seq_len = X_train.shape[1]\n",
    "n_features = 1\n",
    "\n",
    "# Reshape the data for Transformer\n",
    "X_train = X_train.values.reshape(X_train.shape[0], seq_len, n_features)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], seq_len, n_features)\n",
    "\n",
    "# Define the Transformer model\n",
    "inputs = Input(shape=(seq_len, n_features))\n",
    "x = inputs\n",
    "\n",
    "# Add self-attention layers\n",
    "for _ in range(2):\n",
    "    x = tf.keras.layers.MultiHeadAttention(num_heads=8, key_dim=32)(x, x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "# Flatten and add output layer\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "outputs = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[EarlyStopping(patience=5, restore_best_weights=True)])\n",
    "\n",
    "# Evaluate the model\n",
    "mae = history.history['val_loss'][-1]\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "ypred = model.predict(X_test)\n",
    "#For MAE\n",
    "mae = np.mean(np.abs(ypred.ravel() - y_test.ravel()))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "mape = np.mean(np.abs((y_test.ravel() - ypred.ravel()) / y_test.ravel())) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "\n",
    "# For RMSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=mean_squared_error(y_test,ypred) \n",
    "rsme=np.sqrt(mse) \n",
    "print('RSME = {:.2f} (%)'.format(rsme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da21d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#CNN LeNet\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build the LeNet model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(6, kernel_size=5, activation='relu',  input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Conv1D(16, kernel_size=5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(120, activation='relu'))\n",
    "model.add(layers.Dense(84, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='linear'))  # Output layer with linear activation for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "# Make predictions on new data\n",
    "predictions = model.predict(X_test)\n",
    "ypred=predictions\n",
    "print('Predictions:', predictions)\n",
    "ypred = ypred.flatten()\n",
    "\n",
    "\n",
    "#For MAE\n",
    "mae = np.mean(np.abs(ypred - y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "#For MAPE\n",
    "mape = np.mean(np.abs((y_test - ypred) / y_test)) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "mse = mean_squared_error(y_test, ypred) \n",
    "rmse = np.sqrt(mse) \n",
    "print('RMSE = {:.2f} (%)'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LeNet-LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build the LeNet model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(6, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Conv1D(16, kernel_size=5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(120, activation='relu'))\n",
    "model.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Add LSTM layer\n",
    "model.add(layers.Reshape((7, 12)))  # Reshape the output of dense layers for compatibility with LSTM\n",
    "model.add(layers.LSTM(84, return_sequences=False))\n",
    "\n",
    "model.add(layers.Dense(1, activation='linear'))  # Output layer with linear activation for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "# Make predictions on new data\n",
    "predictions = model.predict(X_test)\n",
    "ypred = predictions.flatten()\n",
    "\n",
    "# For MAE\n",
    "mae = np.mean(np.abs(ypred - y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "# For MAPE\n",
    "mape = np.mean(np.abs((y_test - ypred) / y_test)) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "mse = mean_squared_error(y_test, ypred) \n",
    "rmse = np.sqrt(mse) \n",
    "print('RMSE = {:.2f} (%)'.format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e112290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GoogleNet Lstm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build the GoogleNet-LSTM model\n",
    "def inception_module(x, filters):\n",
    "    branch1x1 = layers.Conv1D(filters[0], 1, activation='relu', padding='same')(x)\n",
    "    \n",
    "    branch3x3 = layers.Conv1D(filters[1], 1, activation='relu', padding='same')(x)\n",
    "    branch3x3 = layers.Conv1D(filters[2], 3, activation='relu', padding='same')(branch3x3)\n",
    "    \n",
    "    branch5x5 = layers.Conv1D(filters[3], 1, activation='relu', padding='same')(x)\n",
    "    branch5x5 = layers.Conv1D(filters[4], 5, activation='relu', padding='same')(branch5x5)\n",
    "    \n",
    "    branch_pool = layers.MaxPooling1D(3, strides=1, padding='same')(x)\n",
    "    branch_pool = layers.Conv1D(filters[5], 1, activation='relu', padding='same')(branch_pool)\n",
    "    \n",
    "    output = layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1)\n",
    "    return output\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # Adjust input shape as per your requirements\n",
    "\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "x = layers.Conv1D(6, kernel_size=5, activation='relu', padding='same')(input_layer)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "x = layers.Conv1D(16, kernel_size=5, activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "# Replace fully connected layers with Inception modules\n",
    "x = inception_module(x, [6, 6, 8, 2, 4, 4])\n",
    "x = inception_module(x, [16, 8, 12, 4, 8, 8])\n",
    "\n",
    "x = layers.LSTM(84, return_sequences=True)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = models.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "ypred = predictions.flatten()\n",
    "\n",
    "# For MAE\n",
    "mae = np.mean(np.abs(ypred - y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "# For MAPE\n",
    "mape = np.mean(np.abs((y_test - ypred) / y_test)) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "mse = mean_squared_error(y_test, ypred) \n",
    "rmse = np.sqrt(mse) \n",
    "print('RMSE = {:.2f} (%)'.format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet_LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare the train and test data\n",
    "X_train, y_train = train_data.drop('nat_demand', axis=1), train_data['nat_demand']\n",
    "X_test, y_test = test_data.drop('nat_demand', axis=1), test_data['nat_demand']\n",
    "\n",
    "# Reshape the data for CNN-LSTM\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build the ResNet-LSTM model\n",
    "def residual_block(x, filters):\n",
    "    shortcut = x\n",
    "    \n",
    "    x = layers.Conv1D(filters[0], 1, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv1D(filters[1], 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv1D(filters[2], 1, padding='same')(x)\n",
    "    \n",
    "    shortcut = layers.Conv1D(filters[2], 1, padding='same')(shortcut)\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # Adjust input shape as per your requirements\n",
    "\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "x = layers.Conv1D(6, kernel_size=5, activation='relu', padding='same')(input_layer)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "# Replace Inception modules with residual blocks\n",
    "x = residual_block(x, [6, 6, 8])\n",
    "x = residual_block(x, [6, 6, 8])\n",
    "\n",
    "x = layers.LSTM(84, return_sequences=True)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = models.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "ypred = predictions.flatten()\n",
    "y_test = test_data['nat_demand']\n",
    "y_rlstm = ypred\n",
    "\n",
    "# For MAE\n",
    "mae = np.mean(np.abs(ypred - y_test))\n",
    "print('MAE = {:.2f} (%)'.format(mae))\n",
    "\n",
    "# For MAPE\n",
    "mape = np.mean(np.abs((y_test - ypred) / y_test)) * 100\n",
    "print('MAPE = {:.2f} (%)'.format(mape))\n",
    "\n",
    "# For RMSE\n",
    "mse = mean_squared_error(y_test, ypred) \n",
    "rmse = np.sqrt(mse) \n",
    "print('RMSE = {:.2f} (%)'.format(rmse))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
